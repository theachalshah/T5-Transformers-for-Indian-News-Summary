{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45d98662a7284b92b396eb87ffabb368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad4ef96ec49d4521aa04c1ef6dbdfa7d",
              "IPY_MODEL_c397460deece4e03bb7841df538529cf",
              "IPY_MODEL_9ffb3fd0ef294555aa7ac51220426c29"
            ],
            "layout": "IPY_MODEL_511d36433ff94e93a42df8283f812d0f"
          }
        },
        "ad4ef96ec49d4521aa04c1ef6dbdfa7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_206a12f16a614a99958453ec68622344",
            "placeholder": "​",
            "style": "IPY_MODEL_e59c99632b5b4134861819485545b8e4",
            "value": "Downloading: 100%"
          }
        },
        "c397460deece4e03bb7841df538529cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab6561bb7ec4599b438bdd17aab0dde",
            "max": 1199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edf2853970cd4a128814a55319e67339",
            "value": 1199
          }
        },
        "9ffb3fd0ef294555aa7ac51220426c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c53d199162f4334af9654eb7a57998b",
            "placeholder": "​",
            "style": "IPY_MODEL_952d0f23057148eca06e5901cf47708d",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 87.0kB/s]"
          }
        },
        "511d36433ff94e93a42df8283f812d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206a12f16a614a99958453ec68622344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59c99632b5b4134861819485545b8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ab6561bb7ec4599b438bdd17aab0dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edf2853970cd4a128814a55319e67339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c53d199162f4334af9654eb7a57998b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952d0f23057148eca06e5901cf47708d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2695bb6d39de450a90b3a4bf58dfc6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ec80405e41445679eff3d85d4a2ed37",
              "IPY_MODEL_95bd7d82c30d45479be1cefd3e82552d",
              "IPY_MODEL_0d5a4074e39d4f8bac3433c4b42e80f4"
            ],
            "layout": "IPY_MODEL_917e0353a5464723a760f1c6ce6487ef"
          }
        },
        "7ec80405e41445679eff3d85d4a2ed37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b47150a2b1de497c9d11d1f4801ea3e7",
            "placeholder": "​",
            "style": "IPY_MODEL_ba82e45759024e3e9febebf1a6affa9e",
            "value": "Downloading: 100%"
          }
        },
        "95bd7d82c30d45479be1cefd3e82552d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd78e5430e5407cacb1be88a7d62b45",
            "max": 891691430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0d0249b149841c0b39b72ba6d68efb6",
            "value": 891691430
          }
        },
        "0d5a4074e39d4f8bac3433c4b42e80f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d74eb65cb1684cb0bd5ed37a9bbf067f",
            "placeholder": "​",
            "style": "IPY_MODEL_c9da3f321bf14c4db11b26e28714ff2d",
            "value": " 892M/892M [00:10&lt;00:00, 88.0MB/s]"
          }
        },
        "917e0353a5464723a760f1c6ce6487ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b47150a2b1de497c9d11d1f4801ea3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba82e45759024e3e9febebf1a6affa9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fd78e5430e5407cacb1be88a7d62b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d0249b149841c0b39b72ba6d68efb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d74eb65cb1684cb0bd5ed37a9bbf067f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9da3f321bf14c4db11b26e28714ff2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries\n",
        "1. A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP).\n",
        "2. SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. "
      ],
      "metadata": {
        "id": "v9pZtLT5or7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.9.0\n",
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEm5qbiPhCMA",
        "outputId": "70b364c8-e8e5-41b6-d13d-6a29b63ddd8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.9.0\n",
            "  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==2.9.0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==2.9.0) (2.25.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==2.9.0) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==2.9.0) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==2.9.0) (4.64.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.9.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.9.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.9.0) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0b7fd3d29cd797aeb24c0e1c55d7798209d57717fc529537786636bae2377e11\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.7.0 transformers-2.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries\n",
        "\n",
        "T5 uses Sentencepiece tokenizer, which is implemented in C and is opaque to Python."
      ],
      "metadata": {
        "id": "wugE-6zuq6uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "twhJZPoCkaNA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU Access"
      ],
      "metadata": {
        "id": "Oe--qxbDrVCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH2LCvealaH_",
        "outputId": "4b1317c8-4fca-42a4-d2a9-7e6dfd0f2450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 21 09:22:42 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set GPU if possible else CPU"
      ],
      "metadata": {
        "id": "u7F3mR21rZ61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "metadata": {
        "id": "3evPoNxoljEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class of Custom Dataset to Read the Data Frame and then further load it into Dataloader\n",
        "\n",
        "The output of tokenizer is a dictionary containing two keys – input ids and attention mask. Input ids are the unique identifiers of the tokens in a sentence. Attention mask is used to batch the input sequence together and indicate whether the token should be attended by our model or not.\n",
        "\n",
        "Masking is needed to prevent the attention mechanism of a transformer from “cheating” in the decoder when training. \n",
        "\n",
        "The tokenizer uses the batch_encode_plus method to perform tokenization and generate the necessary outputs, namely: source_id, source_mask from the actual text and target_id and target_mask from the summary text."
      ],
      "metadata": {
        "id": "l4_WguyxrtRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)   \n",
        "\n",
        "    def __getitem__(self, index):       #remove any extra spaces\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')   #pt = pytorch, pad_to_max_length=True for equal size of largest tensor \n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()           \n",
        "        source_mask = source['attention_mask'].squeeze()  # Returns a tensor with all the dimensions of input of size 1 removed\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),  \n",
        "            'source_mask': source_mask.to(dtype=torch.long), # long() is equivalent to self.to(torch. int64)\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "7UgY6KwbltI_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Function\n",
        "\n",
        "The Dataloader passes the data through this train function.\n",
        "\n",
        "A contiguous tensor is a tensor whose elements are stored in a contiguous order without leaving any empty space between them. \n",
        "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
        "\n",
        "clone() creates a copy of tensor that imitates the original tensor's requires_grad field. You should use detach() when attempting to remove a tensor from a computation graph, and clone as a way to copy the tensor while still keeping the copy as a part of the computation graph it came from.\n",
        "\n",
        "language_model_labels (lm_labels) are calculated from the target_ids also, source_id and attention_mask are extracted."
      ],
      "metadata": {
        "id": "1fziiljWuWr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):    \n",
        "        y = data['target_ids'].to(device, dtype = torch.long)  # cuda or cpu\n",
        "        y_ids = y[:, :-1].contiguous()    \n",
        "                                          \n",
        "        lm_labels = y[:, 1:].clone().detach() \n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100  # The id of the padding token\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)   \n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')  #print every 500 training samples\n",
        "        \n",
        "        optimizer.zero_grad()  #zero_grad() restarts looping without losses from the last step if you use the gradient method for decreasing the error (or losses). \n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "vVBjE2LJpANt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate news summary on the data that is unseen during training using Validation Step\n",
        "\n",
        "This is also a required argument that represents a binary mask indicating which tokens in the input should be attended to by the model. The mask has the same shape as input_ids, with 1s indicating tokens that should be attended to and 0s indicating tokens that should be ignored.\n",
        "\n",
        "Number of Beams is the number of different possible sequences considered at each generation step.\n",
        "\n",
        "For regression penalty a higher value for repetition_penalty will make the model less likely to repeat tokens in the output.\n",
        "\n",
        "Length Penalty is an optional argument that encourages the model to generate longer sequences. A value greater than 1 will make the model more likely to generate longer sequences.\n",
        "\n",
        "Special tokens are called special because they are not derived from your input. They are added for a certain purpose and are independent of the specific input."
      ],
      "metadata": {
        "id": "sEld4-GSxADT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0): #the expression enumerate(loader, 0), the second argument 0 is the starting value of the index that enumerate will use to count the items in loader. \n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2, \n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y] \n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')   ##print every 100 validation samples\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "metadata": {
        "id": "PO4zJqWKqlcZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main() Executest the complete Flow\n",
        "\n",
        "PyTorch Data Loader combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\n",
        "\n",
        "The T5ForConditionalGeneration adds a Language Model head to our T5 model. The Language Model head allows us to generate text based on the training of T5 model.\n",
        "\n",
        "The original summary and generated summary are converted into a list and returned to the main function.\n",
        "\n",
        "Both the lists are used to create the final dataframe with 2 columns Generated Summary and Actual Summary."
      ],
      "metadata": {
        "id": "4Fm4oVC7yohj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
        "    VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
        "    TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
        "    VAL_EPOCHS = 1 \n",
        "    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "    SEED = 42               # random seed (default: 42)\n",
        "    MAX_LEN = 512\n",
        "    SUMMARY_LEN = 60 \n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(SEED) # pytorch random seed\n",
        "    np.random.seed(SEED) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    \n",
        "\n",
        "    # Importing and Pre-Processing the domain data\n",
        "    # Selecting the needed columns only. \n",
        "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "    df = pd.read_csv('news_summary.csv',encoding='latin-1')\n",
        "    df = df[['text','ctext']]\n",
        "    df.ctext = 'summarize: ' + df.ctext\n",
        "    print(df.head())\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.8\n",
        "    train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
        "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Training loop\n",
        "    print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "    for epoch in range(TRAIN_EPOCHS):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "    for epoch in range(VAL_EPOCHS):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "        final_df.to_csv('predictions.csv')\n",
        "        print('Output Files generated for review')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636,
          "referenced_widgets": [
            "45d98662a7284b92b396eb87ffabb368",
            "ad4ef96ec49d4521aa04c1ef6dbdfa7d",
            "c397460deece4e03bb7841df538529cf",
            "9ffb3fd0ef294555aa7ac51220426c29",
            "511d36433ff94e93a42df8283f812d0f",
            "206a12f16a614a99958453ec68622344",
            "e59c99632b5b4134861819485545b8e4",
            "6ab6561bb7ec4599b438bdd17aab0dde",
            "edf2853970cd4a128814a55319e67339",
            "1c53d199162f4334af9654eb7a57998b",
            "952d0f23057148eca06e5901cf47708d",
            "2695bb6d39de450a90b3a4bf58dfc6ec",
            "7ec80405e41445679eff3d85d4a2ed37",
            "95bd7d82c30d45479be1cefd3e82552d",
            "0d5a4074e39d4f8bac3433c4b42e80f4",
            "917e0353a5464723a760f1c6ce6487ef",
            "b47150a2b1de497c9d11d1f4801ea3e7",
            "ba82e45759024e3e9febebf1a6affa9e",
            "0fd78e5430e5407cacb1be88a7d62b45",
            "b0d0249b149841c0b39b72ba6d68efb6",
            "d74eb65cb1684cb0bd5ed37a9bbf067f",
            "c9da3f321bf14c4db11b26e28714ff2d"
          ]
        },
        "id": "58sHzMYZGKIi",
        "outputId": "bf8e0270-e5dd-4859-cc6d-463e7c2bfae1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  The Administration of Union Territory Daman an...   \n",
            "1  Malaika Arora slammed an Instagram user who tr...   \n",
            "2  The Indira Gandhi Institute of Medical Science...   \n",
            "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
            "4  Hotels in Maharashtra will train their staff t...   \n",
            "\n",
            "                                               ctext  \n",
            "0  summarize: The Daman and Diu administration on...  \n",
            "1  summarize: From her special numbers to TV?appe...  \n",
            "2  summarize: The Indira Gandhi Institute of Medi...  \n",
            "3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n",
            "4  summarize: Hotels in Mumbai and other Indian c...  \n",
            "FULL Dataset: (4514, 2)\n",
            "TRAIN Dataset: (3611, 2)\n",
            "TEST Dataset: (903, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45d98662a7284b92b396eb87ffabb368"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2695bb6d39de450a90b3a4bf58dfc6ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n",
            "Epoch: 0, Loss:  5.825940132141113\n",
            "Epoch: 0, Loss:  2.218724250793457\n",
            "Epoch: 0, Loss:  3.1664295196533203\n",
            "Epoch: 0, Loss:  2.106602668762207\n",
            "Epoch: 1, Loss:  1.584641695022583\n",
            "Epoch: 1, Loss:  0.8339561223983765\n",
            "Epoch: 1, Loss:  2.1917176246643066\n",
            "Epoch: 1, Loss:  0.7319134473800659\n",
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Output Files generated for review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: -\n",
        "1. https://www.kaggle.com/datasets/sunnysai12345/news-summary?resource=download\n",
        "2. https://github.com/sunnysai12345/News_Summary\n",
        "3. https://wandb.ai/mukilan/T5_transformer/reports/Exploring-Google-s-T5-Text-To-Text-Transformer-Model--VmlldzoyNjkzOTE2\n",
        "4. https://www.youtube.com/watch?v=91iLu6OOrwk\n",
        "5. https://github.com/google/sentencepiece\n",
        "6. https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90#:~:text=T5%20uses%20Sentencepiece%20tokenizer%2C%20which,and%20is%20opaque%20to%20Python.\n",
        "7. https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best#:~:text=The%20output%20of%20tokenizer%20is,by%20our%20model%20or%20not.\n",
        "8. http://jalammar.github.io/illustrated-transformer/\n",
        "9. https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c#:~:text=Masking%20is%20needed%20to%20prevent,a%20translating%20task%20for%20instance).\n",
        "10. https://www.kaggle.com/code/eggwhites2705/transformers-summarization-t5/notebook\n"
      ],
      "metadata": {
        "id": "tFYbHGaS5LRY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BREnT3fUGA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}